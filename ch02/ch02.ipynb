{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1a5c74",
   "metadata": {},
   "source": [
    "## 2.1 Tokenizing Text\n",
    "* 在这个阶段，将text转化为小单元\n",
    "\n",
    "![](assets/tokenizeText.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb0a8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f34e7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
      "\n",
      "Well!--even through t\n"
     ]
    }
   ],
   "source": [
    "with open('the-verdict.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f9c08",
   "metadata": {},
   "source": [
    "* 目标是将这个text tokenize并且embedded\n",
    "* 建立一个简单的tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f067f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.!?;_:\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(\"preprocessed:\", preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24eb45d",
   "metadata": {},
   "source": [
    "* Let's calculate the number of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c4d43b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada38e58",
   "metadata": {},
   "source": [
    "## 2.3 Converting tokens into token IDs\n",
    "* 构建一个词典包含所有的唯一tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2570cb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(\"Total number of unique words:\", len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b779b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n"
     ]
    }
   ],
   "source": [
    "vocab = {word: i for i, word in enumerate(all_words)}\n",
    "for i, word in enumerate(vocab.items()):\n",
    "    if i >= 20:\n",
    "        break;\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d2c23",
   "metadata": {},
   "source": [
    "* 实现一个tokenizer，实现`encode`和`decode`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "142242c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.int_to_str = {i: word for word, i in vocab.items()}\n",
    "        self.unk_token = '<unk>'\n",
    "        self.pad_token = '<pad>'\n",
    "        self.unk_index = vocab.get(self.unk_token, -1)\n",
    "        self.pad_index = vocab.get(self.pad_token, -1)\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.!?;_:\"()\\']|--|\\s)', text)\n",
    "        tokens = [token.strip() for token in tokens if token.strip()]\n",
    "        return [self.vocab.get(token, self.unk_index) for token in tokens]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        text = \" \".join([self.int_to_str.get(token_id, self.unk_token) for token_id in token_ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)  # 将标点符号前的空格去掉\n",
    "        return text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    def __getitem__(self, index):\n",
    "        if index < 0 or index >= len(self.vocab):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        return list(self.vocab.keys())[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d36e4",
   "metadata": {},
   "source": [
    "* `encode`函数将text转为token IDs\n",
    "* `decode`函数将tokens ID转为 text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b82670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c5cf2",
   "metadata": {},
   "source": [
    "* Decode the token IDs back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d743cc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d490600",
   "metadata": {},
   "source": [
    "## 2.4 添加额外的上下文Token\n",
    "![](assets/special_tokens.png \"speical tokens\")\n",
    "\n",
    "* 一些特殊的tokens\n",
    "  * `[BOS]` text的开头\n",
    "  * `[EOS]` text的结尾，用于两个不相关的text分隔\n",
    "  * `[UNk]` 代表没有包含在词典中的word\n",
    "  * `[PAD]` 将short的token扩展为一个batch中最长token的长度的占位\n",
    "* `<|endoftext|>`和`[EOS]`是相同的，GPT-2不使用任何上述的tokens，只使用`<|endoftext|>`来减少复杂度\n",
    "* GPT-2不使用`<|unk|>`，而是使用 *byte pair encoding (BPE)* 来将word分为子word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4869379",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
    "vocab = {word: i for i, word in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08fbc9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcaa9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: word for word, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.!?;_:\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int.get(token, self.str_to_int['<|unk|>']) for token in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        text = \" \".join([self.int_to_str.get(token_id, '<|unk|>') for token_id in token_ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abaf5b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d21df3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfe78b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4edc5",
   "metadata": {},
   "source": [
    "## 2.5 BytePair Encoding\n",
    "* GPT-2使用BPE作为tokenizer\n",
    "* BPE允许将未在vocabulary中定义的word拆分为更小的subword，甚至是独立的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84e54600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f2faaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10011638",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5455fb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35dd7bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd5767f",
   "metadata": {},
   "source": [
    "* BPE将未知的word分割为独立的subword\n",
    "\n",
    "![](assets/subword.png \"subwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e16c4726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 959]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Akwirwier\"\n",
    "tokenIDs = tokenizer.encode(test_text)\n",
    "print(tokenIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d3227e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ak\n",
      "w\n",
      "ir\n",
      "w\n",
      "ier\n"
     ]
    }
   ],
   "source": [
    "for i in tokenIDs:\n",
    "    print(tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a7e2925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akwirwier\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test_strings = tokenizer.decode(tokenIDs)\n",
    "print(test_strings)\n",
    "print(test_strings == test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0191b",
   "metadata": {},
   "source": [
    "## 2.6 Data smpling with sliding window\n",
    "\n",
    "* 现在我们要实现一个简单的data loader，对输入数据集进行迭代，返回inputs和targets\n",
    "\n",
    "![](assets/text_sliding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a3653c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  2.7.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Pytorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f613b3",
   "metadata": {},
   "source": [
    "* 创建dataset和dataloader来从输入text dataset中提取chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02d53b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # 使用滑动窗口来将整个text分割为多个重叠的片段\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunks = token_ids[i:i + max_length]\n",
    "            target_chunks = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunks))\n",
    "            self.target_ids.append(torch.tensor(target_chunks))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e31e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(text, batch_size=4, max_length=256, \n",
    "                      stride=128, shuffle=True, drop_last=True,\n",
    "                      num_workers=0):\n",
    "    # 初始化tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "\n",
    "    # 创建dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a61d3c",
   "metadata": {},
   "source": [
    "* 创建dataloader为batch_size=1，LLM上下文的大小为4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e84a311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb1a00ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  40,  367, 2885, 1464]])\n",
      "Target IDs: tensor([[ 367, 2885, 1464, 1807]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, \n",
    "                                  stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(\"Input IDs:\", first_batch[0])\n",
    "print(\"Target IDs:\", first_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9fa0ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String: I HAD always\n",
      "Target String:  HAD always thought\n"
     ]
    }
   ],
   "source": [
    "input_str = tokenizer.decode(first_batch[0][0].tolist())\n",
    "print(\"Input String:\", input_str)\n",
    "target_str = tokenizer.decode(first_batch[1][0].tolist())\n",
    "print(\"Target String:\", target_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231e0b0",
   "metadata": {},
   "source": [
    "## 2.7Creating token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2e7e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb0de2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabe_size = 6\n",
    "out_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocabe_size, out_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abfbe94",
   "metadata": {},
   "source": [
    "* embedding_layer 的weights形状为6×3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cfefc556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_layer.weights: Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding_layer.weights:\", embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec21c28",
   "metadata": {},
   "source": [
    "## 2.8 Encoding word position\n",
    "* BPE的字典长度为50257\n",
    "* 将token encode 为256维的vector 表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2e5d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f93b2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length,\n",
    "                                  stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a13c2d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31e88b",
   "metadata": {},
   "source": [
    "* GPT-2 使用absolute position embedding，创建另一个embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9fe05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d720e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ccc81f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99a6b723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c30784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
